{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-End Corporate Credit Risk Rating System (PySpark)\n",
        "\n",
        "This notebook implements a complete credit risk pipeline suitable for Databricks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Configuration\n",
        "RATING_MAPPING = {\n",
        "    \"AAA\": 0.0005,\n",
        "    \"AA\": 0.001,\n",
        "    \"A\": 0.002,\n",
        "    \"BBB\": 0.005,\n",
        "    \"BB\": 0.01,\n",
        "    \"B\": 0.03,\n",
        "    \"CCC\": 0.08,\n",
        "    \"CC\": 0.15,\n",
        "    \"C\": 0.25,\n",
        "    \"D\": 1.00\n",
        "}\n",
        "\n",
        "FEATURES_NUMERIC = [\n",
        "    \"revenue\", \"total_assets\", \"total_liabilities\", \"ebitda\", \"interest_expense\",\n",
        "    \"debt_to_equity\", \"current_ratio\", \"interest_coverage\", \"roa\", \"age\"\n",
        "]\n",
        "FEATURES_CATEGORICAL = [\"industry\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Synthetic Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def generate_synthetic_data(n_rows=10000, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    data = []\n",
        "    industries = ['Technology', 'Manufacturing', 'Retail', 'Healthcare', 'Energy', 'Finance']\n",
        "    \n",
        "    for i in range(n_rows):\n",
        "        company_id = f\"COMP_{i:06d}\"\n",
        "        industry = random.choice(industries)\n",
        "        \n",
        "        # Base financials\n",
        "        assets = np.exp(np.random.normal(15, 1.5))\n",
        "        is_risky = random.random() < 0.15\n",
        "        \n",
        "        if is_risky:\n",
        "            liabilities_ratio = np.random.uniform(0.6, 1.5)\n",
        "            profit_margin = np.random.uniform(-0.1, 0.05)\n",
        "            coverage_factor = np.random.uniform(0, 2)\n",
        "        else:\n",
        "            liabilities_ratio = np.random.uniform(0.2, 0.7)\n",
        "            profit_margin = np.random.uniform(0.05, 0.25)\n",
        "            coverage_factor = np.random.uniform(3, 15)\n",
        "\n",
        "        liabilities = assets * liabilities_ratio\n",
        "        revenue = assets * np.random.uniform(0.5, 2.0)\n",
        "        ebitda = revenue * profit_margin\n",
        "        interest_expense = liabilities * 0.05 \n",
        "        current_assets = assets * np.random.uniform(0.3, 0.6)\n",
        "        cl_ratio = np.random.uniform(0.6, 0.9) if is_risky else np.random.uniform(0.3, 0.5)\n",
        "        current_liabilities = liabilities * cl_ratio\n",
        "        founding_year = np.random.randint(1950, 2020)\n",
        "        \n",
        "        default = 0\n",
        "        if liabilities > assets: default = 1\n",
        "        current_ratio = current_assets / current_liabilities if current_liabilities > 0 else 0\n",
        "        interest_coverage = ebitda / interest_expense if interest_expense > 0 else 0\n",
        "        if current_ratio < 0.9 and interest_coverage < 1.2: default = 1\n",
        "        if is_risky and random.random() < 0.1: default = 1\n",
        "            \n",
        "        row = (\n",
        "            company_id, industry, float(assets), float(liabilities), float(current_assets),\n",
        "            float(current_liabilities), float(revenue), float(ebitda), float(interest_expense),\n",
        "            int(founding_year), int(default)\n",
        "        )\n",
        "        data.append(row)\n",
        "        \n",
        "    schema = StructType([\n",
        "        StructField(\"company_id\", StringType(), True),\n",
        "        StructField(\"industry\", StringType(), True),\n",
        "        StructField(\"total_assets\", DoubleType(), True),\n",
        "        StructField(\"total_liabilities\", DoubleType(), True),\n",
        "        StructField(\"current_assets\", DoubleType(), True),\n",
        "        StructField(\"current_liabilities\", DoubleType(), True),\n",
        "        StructField(\"revenue\", DoubleType(), True),\n",
        "        StructField(\"ebitda\", DoubleType(), True),\n",
        "        StructField(\"interest_expense\", DoubleType(), True),\n",
        "        StructField(\"founding_year\", IntegerType(), True),\n",
        "        StructField(\"default\", IntegerType(), True)\n",
        "    ])\n",
        "    \n",
        "    # Create Spark DataFrame directly\n",
        "    return spark.createDataFrame(data, schema)\n",
        "\n",
        "# Generate and Display\n",
        "print(\"Generating Data...\")\n",
        "df = generate_synthetic_data(10000)\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "def calculate_financial_ratios(df):\n",
        "    df = df.withColumn(\"equity\", F.col(\"total_assets\") - F.col(\"total_liabilities\"))\n",
        "    df = df.withColumn(\"debt_to_equity\", F.when(F.col(\"equity\") == 0, 999.0).otherwise(F.col(\"total_liabilities\") / F.col(\"equity\")))\n",
        "    df = df.withColumn(\"current_ratio\", F.when(F.col(\"current_liabilities\") == 0, 999.0).otherwise(F.col(\"current_assets\") / F.col(\"current_liabilities\")))\n",
        "    df = df.withColumn(\"interest_coverage\", F.when(F.col(\"interest_expense\") == 0, 999.0).otherwise(F.col(\"ebitda\") / F.col(\"interest_expense\")))\n",
        "    df = df.withColumn(\"roa\", F.when(F.col(\"total_assets\") == 0, 0.0).otherwise(F.col(\"ebitda\") / F.col(\"total_assets\")))\n",
        "    df = df.withColumn(\"age\", F.lit(2025) - F.col(\"founding_year\"))\n",
        "    return df\n",
        "\n",
        "def build_feature_pipeline(categorical_cols, numerical_cols):\n",
        "    stages = []\n",
        "    for cat_col in categorical_cols:\n",
        "        indexer = StringIndexer(inputCol=cat_col, outputCol=f\"{cat_col}_indexed\", handleInvalid=\"keep\")\n",
        "        encoder = OneHotEncoder(inputCols=[f\"{cat_col}_indexed\"], outputCols=[f\"{cat_col}_vec\"])\n",
        "        stages += [indexer, encoder]\n",
        "    \n",
        "    assembler_inputs = [f\"{col}_vec\" for col in categorical_cols] + numerical_cols\n",
        "    assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
        "    stages.append(assembler)\n",
        "    return Pipeline(stages=stages)\n",
        "\n",
        "# Apply\n",
        "df_ratios = calculate_financial_ratios(df)\n",
        "feature_pipeline = build_feature_pipeline(FEATURES_CATEGORICAL, FEATURES_NUMERIC)\n",
        "feature_model = feature_pipeline.fit(df_ratios)\n",
        "df_features = feature_model.transform(df_ratios)\n",
        "\n",
        "display(df_features.select(\"company_id\", \"features\", \"default\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Modeling (Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "\n",
        "train_df, test_df = df_features.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"default\", numTrees=20)\n",
        "model = rf.fit(train_df)\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "display(predictions.select(\"company_id\", \"default\", \"probability\", \"prediction\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "# AUC\n",
        "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"default\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "print(f\"AUC: {auc}\")\n",
        "\n",
        "# Accuracy\n",
        "multiclass_evaluator = MulticlassClassificationEvaluator(labelCol=\"default\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = multiclass_evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Risk Scoring & Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "def normalize_probability(probability_vector):\n",
        "    try: return float(probability_vector[1])\n",
        "    except: return 0.0\n",
        "\n",
        "extract_pd_udf = F.udf(normalize_probability, DoubleType())\n",
        "\n",
        "def calculate_risk_score_and_rating(df):\n",
        "    df = df.withColumn(\"pd\", extract_pd_udf(F.col(\"probability\")))\n",
        "    df = df.withColumn(\"risk_score\", (1 - F.col(\"pd\")) * 1000)\n",
        "    \n",
        "    # Rating Logic\n",
        "    sorted_rating = sorted(RATING_MAPPING.items(), key=lambda x: x[1])\n",
        "    expr = None\n",
        "    for rating, threshold in sorted_rating:\n",
        "        if expr is None: expr = F.when(F.col(\"pd\") <= threshold, rating)\n",
        "        else: expr = expr.when(F.col(\"pd\") <= threshold, rating)\n",
        "    expr = expr.otherwise(\"D\")\n",
        "    \n",
        "    return df.withColumn(\"rating\", expr)\n",
        "\n",
        "scored_df = calculate_risk_score_and_rating(predictions)\n",
        "display(scored_df.select(\"company_id\", \"pd\", \"risk_score\", \"rating\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Migration Matrix Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def generate_migration_matrix(df):\n",
        "    df = df.withColumn(\"noise\", F.randn() * 0.02)\n",
        "    df = df.withColumn(\"future_pd\", F.col(\"pd\") + F.col(\"noise\"))\n",
        "    df = df.withColumn(\"future_pd\", F.when(F.col(\"future_pd\") < 0, 0.0).when(F.col(\"future_pd\") > 1, 1.0).otherwise(F.col(\"future_pd\")))\n",
        "    \n",
        "    # Re-apply rating logic on future PD requires duplicating logic or UDF\n",
        "    # For notebook simplicity, we repeat logic\n",
        "    sorted_rating = sorted(RATING_MAPPING.items(), key=lambda x: x[1])\n",
        "    expr = None\n",
        "    for rating, threshold in sorted_rating:\n",
        "        if expr is None: expr = F.when(F.col(\"future_pd\") <= threshold, rating)\n",
        "        else: expr = expr.when(F.col(\"future_pd\") <= threshold, rating)\n",
        "    expr = expr.otherwise(\"D\")\n",
        "    \n",
        "    df = df.withColumn(\"rating_t1\", expr)\n",
        "    return df.stat.crosstab(\"rating\", \"rating_t1\")\n",
        "\n",
        "matrix = generate_migration_matrix(scored_df)\n",
        "display(matrix)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}